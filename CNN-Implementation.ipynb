{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The character recognition problem is implemented and compared with two algorithms / techniques with lots of preprocessing. The packages used to accomplish this task are cv2, NumPy, mahotas for preprocessing. Scikit-learn for feature extraction and implementing SVM (support vector machines) and Keras for implementing a CNN (Convolutional Neural Net). I am attaching two jupyter notebooks for this submission one for each algorithm. Just replace the file paths to chars74k-lite dataset to train and detection images 1 and 2 to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mahotas\n",
    "import glob\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from skimage import feature, exposure\n",
    "import imutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp_chars = '/Desktop/chars74k-lite'\n",
    "fp_detect1 = '/Desktop/detection-images/detection-1.jpg'\n",
    "fp_detect2 = '/Desktop/detection-images/detection-2.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After locating files with help of glob package, we read with opencv module and convert the images grayscale 3  channel to grayscale 1 channel. As first steps to preprocessing and feature extraction we try to remove noise, blurr the image to apply thresholding. we use otsu thresholding here as it working well after experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "images = []\n",
    "labels = []\n",
    "for i in glob.glob(fp_chars+'/*'):\n",
    "    for file in glob.glob(i+'/*.jpg'):\n",
    "        #read the file in cv2\n",
    "        image = cv2.imread(file)\n",
    "        #convert from 3 channel to 1 channel\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        #remove noise\n",
    "        image = cv2.fastNlMeansDenoising(image)\n",
    "        #blur the image with 5,5 window\n",
    "        image = cv2.GaussianBlur(image,(5,5),0)\n",
    "        #Thresholding with otsu\n",
    "        T = mahotas.thresholding.otsu(image)\n",
    "        img = image.copy()\n",
    "        img[img>T]=255\n",
    "        img[img<255]=0\n",
    "        img=cv2.bitwise_not(img) \n",
    "        #add image the list\n",
    "        images.append(img)\n",
    "        #add label of the image \n",
    "        labels.append(file[39])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[6000])\n",
    "plt.show()\n",
    "labels[6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_training_score(history):\n",
    "  print('Availible variables to plot: {}'.format(history.history.keys()))\n",
    "  for key in history.history.keys():\n",
    "    print(key)\n",
    "    plt.plot(history.history[key])\n",
    "    plt.title('model ' + key)\n",
    "    plt.ylabel(key)\n",
    "    plt.xla\n",
    "    bel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imarray = np.asarray(images)\n",
    "uniq_labels = list(set(labels))\n",
    "labels_int = [uniq_labels.index(i) for i in labels]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(imarray, labels_int, shuffle=True, random_state=42)\n",
    "\n",
    "# Split x_train and y_train into training and validation\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, shuffle=True, random_state=42)\n",
    "\n",
    "shape_train = x_train.shape\n",
    "shape_validate = x_validate.shape\n",
    "shape_test = x_test.shape\n",
    "\n",
    "# normalize the data\n",
    "# Flatten all images (Both training and testing images)\n",
    "x_train_flatten = x_train.reshape(shape_train[0], shape_train[1], shape_train[2], 1)\n",
    "x_train_flatten = x_train_flatten.astype('float32')/255\n",
    "\n",
    "x_val_flatten = x_validate.reshape(shape_validate[0], shape_validate[1], shape_validate[2], 1)\n",
    "x_val_flatten = x_val_flatten.astype('float32')/255\n",
    "\n",
    "x_test_flatten = x_test.reshape(shape_test[0], shape_test[1], shape_test[2], 1)\n",
    "x_test_flatten = x_test_flatten.astype('float32')/255\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_validate = to_categorical(y_validate)\n",
    "\n",
    "# Create the model\n",
    "def conv_model(img_width, img_height, channels):\n",
    "    model = Sequential()  # Initalize a new model\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (3,3),activation='relu', input_shape=(img_width, img_height, channels) ))\n",
    "    model.add(layers.Conv2D(256, (3,3),activation='relu'))\n",
    "    model.add(layers.Conv2D(512, (3,3),activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.35))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dense(26, activation='softmax'))  \n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = conv_model(20,20,1)\n",
    "\n",
    "# Train the model, remeber to create a validation set\n",
    "\n",
    "history = model.fit(x_train_flatten, y_train, epochs = 10, batch_size=16, validation_data = (x_val_flatten, y_validate))\n",
    "\n",
    "# Plot the training using the helper function created in task 0\n",
    "\n",
    "plot_training_score(history)\n",
    "\n",
    "# Evaluate your model and print the score of the test set\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test_flatten, y_test)\n",
    "score = test_acc\n",
    "\n",
    "print('Test Accuracy :',score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detection(fp, clf, size=(20,20), model=model):\n",
    "        image = cv2.imread(fp)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # blur the image, find edges, and then find contours along\n",
    "        # the edged regions\n",
    "        blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "        edged = cv2.Canny(blurred, 30, 150)\n",
    "        (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # sort the contours by their x-axis position, ensuring\n",
    "        # that we read the characters from left to right\n",
    "        cnts = sorted([(c, cv2.boundingRect(c)[0]) for c in cnts], key = lambda x: x[1])\n",
    "\n",
    "        # loop over the contours\n",
    "        for (c, _) in cnts:\n",
    "            # compute the bounding box for the rectangle\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "            # if the width is at least 5 pixels and the height\n",
    "            # is at least 10 pixels, the contour is likely a digit\n",
    "            if w >= 5 and h >= 10:\n",
    "                # crop the ROI and then threshold the grayscale\n",
    "                roi = image[y:y + h, x:x + w]\n",
    "                roi = cv2.resize(roi, size)\n",
    "                roi = cv2.fastNlMeansDenoising(roi)\n",
    "                thresh = roi.copy()\n",
    "                T = mahotas.thresholding.otsu(image)\n",
    "                thresh[thresh > T] = 255\n",
    "                thresh[thresh < 255] = 0\n",
    "                thresh = cv2.bitwise_not(thresh)\n",
    "                if clf=='svm':\n",
    "                    hist = hog(thresh)\n",
    "                    char = uniq_labels[model.predict([hist])[0]]\n",
    "                elif clf=='cnn':\n",
    "                    thresh = thresh/255.\n",
    "                    thresh = thresh.reshape(1, size[0], size[1], 1)\n",
    "                    char = model.predict_classes(thresh)\n",
    "                    char = uniq_labels[int(char)]\n",
    "                #print(\"I think that character is: {}\".format(char))\n",
    "\n",
    "                # draw a rectangle around the char, the show what the character is\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "                cv2.putText(image, str(char), (x - 10, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "        plt.figure(figsize=(18,24))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detection(fp_detect1, clf='cnn', model=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detection(fp_detect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
